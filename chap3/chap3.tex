\chapter{提案手法}
\thispagestyle{empty}
\label{chap3}
\minitoc

\newpage

\section{はじめに}
本章では，提案手法の概要，シミュレーション環境について述べる．続く3.2節では，提案手法の詳細について述べる．
\section{提案手法}\label{3 method}
本研究の目的は，Q学習を基礎とした強化学習アルゴリズム，新たな知識選択型転移強化学習メカニズムであるSAP-netを組み合わせることにより，ロボットが複雑な障害物環境下での適応と学習をより効率的に行えるようにする新しいアプローチを開発し，その有効性を実験的に検証することにある．このアプローチは，ロボットが未知の障害物環境に迅速に適応，効果的な回避策を学習し，適用する能力を大幅に向上させることを目指している．この目的を達成するために，高度な物理演算を実現するシミュレータWebotsを使用し，ロボットが様々な障害物配置を含む環境下での行動策を効率的に学習できるかどうかを検証する．

\subsection{Q学習による強化学習実験}
初めに複数の配置パターンの障害物配置が含まれる環境下でQ学習による強化学習実験を行う．Q学習は，エージェントが取るべき最適な行動を学習するための一種の価値ベースの強化学習手法であり，各状態における行動の価値（Q値）を推定することにより，最適なポリシーを導出することを目指す．この実験では，ロボットが未知の障害物環境に置かれた際に，自己の位置から目標地点までの経路を最適化する過程を学習する．実験の初期段階では、ロボットはランダムな行動を取ることから始まり，その結果として得られる報酬を基に，徐々に最適な行動方針を学習していく．このプロセスの中心にあるのは，行動の結果として得られる即時報酬と将来の報酬の合計を最大化することによって，最適な行動選択を行うことである．

\subsection{SAP-netの活用}
SAP-netはロボットの初期座標を基準に障害物までの角度と距離を保存させる．この情報はロボットが環境を理解し，障害物を正確に把握するための基盤となる．次に強化学習から得た行動価値関数の情報を取得し，それらの類似度を計算する．類似度計算は異なる状況における最適な行動の一貫性を確認する重要なステップである．そして計算された類似度情報を基にネットワークを構築する．このネットワークは活性化拡散モデルとして機能し，異なる行動価値関数を統合し，環境への柔軟な対応を可能にする．そして構築されたネットワークは知識を選択し，動的な状況において障害物回避の戦略を提供する．これによりロボットは瞬時の判断により，適切な知識を活用して効果的な障害物回避を実現する．

\subsection{物理演算シミュレーション}
強化学習は試行錯誤的に行動を何千と繰り返して学習するため，実環境でやると時間がかかってしまう．また実機で使用するロボットが破損する恐れがある．そこでWebotsという物理演算シミュレーションを使用する．[Webots1998]．Webotsの操作画面をFig．\ref{webots_stage}に示す．Webotsを使用することで，シミュレーション内のロボットが強化学習を行っていくため，実環境で行うよりも安全に効率よく学習することが可能である．使用するロボットモデルをFig.\ref{model}に示す．また障害物の認識はLIDARを使用する．
\begin{figure}[h]
	\centering
	\includegraphics[width=80mm]{./chap2/chap2_fig/webots_stage}
	\caption{Webotsの操作画面}
	\label{webots_stage}
\end{figure}
\begin{figure}[h]
	\centering
	\includegraphics[width=40mm]{./chap2/chap2_fig/model}
	\caption{シミュレーションで用いるロボットモデル}
	\label{model}
\end{figure}

\section{おわりに}
本章では提案手法について述べた．\ref{3 method}節では提案手法として，Q学習による強化学習実験，SAP-netの活用方法，物理演算シミュレーションについて述べた．次章では提案手法に基づいた実験について述べる．



\clearpage
\newpage



