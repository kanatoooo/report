\chapter{実験}
\thispagestyle{empty}
\label{chap4}
\minitoc

\newpage
\section{はじめに}
本章では，提案したアプローチの有用性を示すために行った実験について述べる．\ref{4 RLsim}節ではWebotsで障害物回避の強化学習シミュレーションによる実験の目的，実験条件，および実験結果について述べる．\ref{4 SAP-net}節では，SAP-netを実装したシミュレーション実験の目的，実験条件，および実験結果について述べる．\ref{4 zikki}節では学習結果を転移させた実機ロボットによる障害物回避の実験の目的，条件，および結果について述べる．

\clearpage

\section{障害物回避の強化学習実験}\label{4 RLsim}
\subsection{目的と実験条件}
事前実験として強化学習を用いて障害物を回避する実験を行った．事前実験の目的は障害物回避の知識を実機に転移させるためである．本実験の条件および学習パラメータをTable.4.1に示す．実験環境をFig.\ref{zikkennkannkyou1}に示す．

\begin{table}[htb]
\centering
  \caption{強化学習の条件および学習パラメータ}
  \scalebox{0.8}{
  \begin{tabular}{|c|c|}  \hline
     エピソード数（回） & 7000  \\ \hline 
     ステップ数 & 4（前進・後退・右旋回・左旋回） \\ \hline 
     強化学習の手法 & Q学習 \\ \hline 
     行動価値の配列数 & 5（x座標，y座標，ロボットの向き，障害物までの角度，障害物までの最小距離）\\ \hline 
     正の報酬の付与条件 & ゴール到達時\\ \hline 
     負の報酬の付与条件 & 障害物に衝突またはゴールから遠ざかる行動をした時\\ \hline 
     学習率 & 0.1\\ \hline 
     割引率 & 0.9\\ \hline 
     ボルツマン選択の温度定数 & 0.5\\ \hline 
  \end{tabular}
  }
\end{table} 
 
\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/zikkennkannkyou1}
	\caption{強化学習シミュレーションの環境}
	\label{zikkennkannkyou1}
\end{figure}
上記の条件・学習パラメータ・環境で強化学習を進め，エピソードごとのステップ数を分析して，障害物の回避策を効率的に実施するための知識を構築する．
\subsection{実験結果}
以下にFig.\ref{zikkennkannkyou1}の配置パターン(1番から10番)の強化学習の結果を示す．Fig.\ref{gakusyuu1}〜Fig.\ref{gakusyuu10}に行動回数の推移を表した学習曲線，Fig.\ref{housyuu1}〜Fig.\ref{housyuu10}に獲得報酬の推移を表した学習曲線を示す．
\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/gakusyuu1}
	\caption{行動回数の推移を表した学習曲線（1番）}
	\label{gakusyuu1}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/gakusyuu2}
	\caption{行動回数の推移を表した学習曲線（2番）}
	\label{gakusyuu2}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/gakusyuu3}
	\caption{行動回数の推移を表した学習曲線（3番）}
	\label{gakusyuu3}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/gakusyuu4}
	\caption{行動回数の推移を表した学習曲線（4番）}
	\label{gakusyuu4}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/gakusyuu5}
	\caption{行動回数の推移を表した学習曲線（5番）}
	\label{gakusyuu5}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/gakusyuu6}
	\caption{行動回数の推移を表した学習曲線（6番）}
	\label{gakusyuu6}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/gakusyuu7}
	\caption{行動回数の推移を表した学習曲線（7番）}
	\label{gakusyuu7}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/gakusyuu8}
	\caption{行動回数の推移を表した学習曲線（8番）}
	\label{gakusyuu8}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/gakusyuu9}
	\caption{行動回数の推移を表した学習曲線（9番）}
	\label{gakusyuu9}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/gakusyuu10}
	\caption{行動回数の推移を表した学習曲線（10番）}
	\label{gakusyuu10}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/housyuu1}
	\caption{獲得報酬の推移を表した学習曲線（1番）}
	\label{housyuu1}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=140mm]{./chap4/chap4_fig/TLreward}
	\caption{獲得報酬の推移を表した学習曲線（2番）}
	\label{TLreward}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/housyuu3}
	\caption{行動回数の推移を表した学習曲線（3番）}
	\label{housyuu3}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/housyuu4}
	\caption{行動回数の推移を表した学習曲線（4番）}
	\label{housyuu4}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/housyuu5}
	\caption{行動回数の推移を表した学習曲線（5番）}
	\label{housyuu5}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/housyuu6}
	\caption{獲得報酬の推移を表した学習曲線（6番）}
	\label{housyuu6}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/housyuu7}
	\caption{獲得報酬の推移を表した学習曲線（7番）}
	\label{housyuu7}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/housyuu8}
	\caption{行動回数の推移を表した学習曲線（8番）}
	\label{housyuu8}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/housyuu9}
	\caption{行動回数の推移を表した学習曲線（9番）}
	\label{housyuu9}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/housyuu10}
	\caption{行動回数の推移を表した学習曲線（10番）}
	\label{housyuu10}
\end{figure} 
\clearpage
Fig.\ref{gakusyuu1}〜Fig.\ref{gakusyuu10}より，エピソード数が増えるにつれてステップ数が減少していることがわかる．このことからアルゴリズムの通り，探索しながら少ないステップ数で障害物を回避してゴールしていることを示している．Fig.\ref{housyuu1}〜Fig.\ref{housyuu10}より，エピソード数が増えるにつれて獲得報酬は正の値に収束傾向であることがわかる．このことから最適な行動戦略やポリシーを獲得し，報酬を最大化していることがわかる．以下に強化学習後のシミュレーションロボットを1エピソードだけ実行したので，10パターンの障害物回避の移動軌跡をFig.\ref{kiseki1}〜Fig.\ref{kiseki10}に示す．\\
\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/kiseki1}
	\caption{強化学習後の移動軌跡（1番）}
	\label{kiseki1}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/TLkiseki}
	\caption{強化学習後の移動軌跡（2番）}
	\label{TLkiseki}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/kiseki3}
	\caption{強化学習後の移動軌跡（3番）}
	\label{kiseki3}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/kiseki4}
	\caption{強化学習後の移動軌跡（4番）}
	\label{kiseki4}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/kiseki5}
	\caption{強化学習後の移動軌跡（5番）}
	\label{kiseki5}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/kiseki6}
	\caption{強化学習後の移動軌跡（6番）}
	\label{kiseki6}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/kiseki7}
	\caption{強化学習後の移動軌跡（7番）}
	\label{kiseki7}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/kiseki8}
	\caption{強化学習後の移動軌跡（8番）}
	\label{kiseki8}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/kiseki9}
	\caption{強化学習後の移動軌跡（9番）}
	\label{kiseki9}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/kiseki10}
	\caption{強化学習後の移動軌跡（10番）}
	\label{kiseki10}
\end{figure}

\clearpage


Fig.\ref{kiseki1}〜Fig.\ref{kiseki10}より，それぞれの配置パターンで障害物を回避するような移動軌跡が示せた．よって障害物を回避する行動策を学習することができた．また10パターンの障害物回避の強化学習の結果から，Fig.\ref{zikkennkannkyou1}の配置を参考に，Fig.\ref{POLICY}に示すような知識番号と知識間の重みが書かれたSAP-netの知識（方策）ネットワークを作成した．例えば障害物が前方にない場合は，1番が選択されて直進をするような知識が選択される．障害物が5番の位置にある場合，知識番号5番が選択されて障害物を右に回避するような行動をとる．つまり，Fig.\ref{kiseki5}のような障害物を右に回避するような行動をとる．障害物が4番の位置にある場合，4番が選択されて左を回避するような行動をとる．つまりFig.\ref{kiseki4}のような障害物を左に回避するような行動をとる．
\begin{figure}[h]
	\centering
	\includegraphics[width=100mm]{./chap4/chap4_fig/POLICY}
	\caption{SAP-netの知識（方策）ネットワークのイメージ図}
	\label{POLICY}
\end{figure}
\clearpage
\newpage
\section{SAP-netを実装したシミュレーション実験}\label{4 SAP-net}
\subsection{目的と実験条件}
前節にて述べた障害物回避の強化学習実験においては，Webotsを用いたシミュレーション環境でQ学習を基にした基本的な障害物回避の学習プロセスを実施し，ロボットが異なる障害物配置パターン下での効率的な回避行動を学習することができることを示した．これらの実験を通じて得られた知識は，ロボットが複雑な環境下での障害物回避能力を向上させるための基礎となる．本節では前節で得られた10個の知識（方策）をSAP-netの知識ネットワークの基盤として，さらに進んだ学習アルゴリズムの適用を行う．具体的には，SAP-netを通じて，ロボットが障害物までの角度と距離，ロボットの初期座標を考慮した上で，これまでに学習した知識（方策）と現在の環境との類似度を計算し，最適な行動選択を行うシミュレーションを実装する．障害物を中心に配置した環境でシミュレーションを行う．
\subsection{実験結果}
Fig.\ref{SAP-net_sim}に行動回数の推移を表した学習曲線を，Fig.\ref{SAP-net_reward}に獲得報酬の推移を表した学習曲線を示す．
\newpage
\begin{figure}[h]
	\centering
	\includegraphics[width=110mm]{./chap4/chap4_fig/SAP-net_sim}
	\caption{行動回数の推移を表した学習曲線}
	\label{SAP-net_sim}
\end{figure}
\begin{figure}[h]
	\centering
	\includegraphics[width=110mm]{./chap4/chap4_fig/SAP-net_reward}
	\caption{獲得報酬の推移を表した学習曲線}
	\label{SAP-net_reward}
\end{figure}　

Fig.\ref{SAP-net_sim}より，知識（方策）を保持した状態なので，強化学習実験とは違ってより少ない行動でゴールしていることがわかる．Fig.\ref{SAP-net_reward}より，Webotsでのバグが生じたものの，ほぼ正の値に収束していることがわかる．よって知識選択型転移強化学習を用いることで障害物回避の有用性を示した．またFig.4.35のような，障害物を2つ配置した場合の行動も検証したので，1エピソードだけ実行して，その時の移動軌跡をFig.\ref{SAP-net_kiseki}に示す．\\
\begin{figure}[h]
	\centering
	\includegraphics[width=110mm]{./chap4/chap4_fig/SAP-net_sim2}
	\caption{障害物を2つ配置した場合のシミュレーション環境}
	\label{SAP-net_simi2}
\end{figure}
\begin{figure}[h]
	\centering
	\includegraphics[width=110mm]{./chap4/chap4_fig/SAP-net_kiseki}
	\caption{障害物を2つ配置した場合の移動軌跡}
	\label{SAP-net_kiseki}
\end{figure}　

2つの障害物をLIDARが認識しながら障害物を回避していることが示せた．よって知識選択型転移強化学習を用いることで複数の障害物も回避することの有用性を示した．
\clearpage
\section{実機実装と障害物回避性能の検証}\label{4 zikki}
\subsection{目的と実験条件}
本節では前節で得られた10個の知識（方策）とSAP-netを実機である株式会社RT製ラズベリーパイマウスに転移させて，前節と同じような環境で障害物回避の実現を目的とする．具体的にはWebotsシミュレーションで得られた10個の知識（方策）とSAP-netを，ラズベリーパイマウスという実機に転移させることで，実際の物理環境で障害物回避戦略の有効性を検証する．この実機実装の過程には，シミュレーションから実機へのパラメータ調整を含む，ラズベリーパイマウスにSAP-netアルゴリズムを組み込み，実環境での障害物回避の実現を目指す．実機実験では，シミュレーション環境と同等の条件下で物理的に障害物の配置を再現し，ラズベリーパイマウスが効率的に障害物を回避しつつ目標地点に到達できるかを評価する．使用する実機はFig.\ref{raspimouse}に示すように，Slamtec製RPLIDARを接続したラズベリーパイマウスを使用する．また，実験エリアをFig.\ref{zikeneria}示す．
\begin{figure}[h]
	\centering
	\includegraphics[width=60mm]{./chap4/chap4_fig/raspimouse}
	\caption{ラズベリーパイマウス}
	\label{raspimouse}
\end{figure}
\begin{figure}[h]
	\centering
	\includegraphics[width=60mm]{./chap4/chap4_fig/zikeneria}
	\caption{実験環境}
	\label{zikeneria}
\end{figure}
\subsection{実験結果}
Fig.\ref{syoumigikaihi}に障害物を右に回避した時の比較動合成画像を，Fig.\ref{syouhidarikaihi}に障害物を左に回避した時の比較動合成画像を示す．
\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/syoumigikaihi}
	\caption{障害物を右に回避した時の比較動合成画像}
	\label{syoumigikaihi}
\end{figure}
\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/syouhidarikaihi}
	\caption{障害物を右に回避した時の比較動合成画像}
	\label{syouhidarikaihi}
\end{figure}

Fig.\ref{syoumigikaihi}およびFig.\ref{syouhidarikaihi}より，知識を選択しながら障害物を回避することに成功した．また，Fig.\ref{POLICY}から，ステップ数においての知識番号選択のグラフをFig.\ref{migikaihi}とFig.\ref{hidarikaihi}に示す．

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/migikaihi}
	\caption{障害物を右に回避した時の知識選択の推移}
	\label{migikaihi}
\end{figure}
\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/hidarikaihi}
	\caption{障害物を左に回避した時の知識選択の推移}
	\label{hidarikaihi}
\end{figure}
Fig.\ref{migikaihi}より障害物を右に回避する際，Fig.\ref{POLICY}の3番や9番が選択されていることがわかる．3番や9番は障害物を右に回避する際に選択されやすくなる番号である．またFig.\ref{hidarikaihi}より障害物を左に回避する際，6番や8番が選択されていることがわかる．6番や8番は障害を左に回避する際に選択されやすくなる番号である．このことから環境情報を入力として知識を選択しながら障害物を回避していることが示せた．また，初期位置でのセットアップで方向の誤差により，進行方向が左右に少しずれて，選択する知識が変更になり，同時に障害物を回避する方向が変わったことが示せた．

\newpage
\clearpage
\section{おわりに}\label{4 last}
本章では，提案手法に基づいた実験の詳細について述べた．\ref{4 RLsim}節では，強化学習実験の目的，条件，および結果について述べた．\ref{4 SAP-net}節では，SAP-netを実装したシミュレーション実験の目的，条件，および結果について述べた．\ref{4 zikki}節では，強化学習で獲得した知識を実機に転移させ，またSAP-netを実装させることで，実環境で知識を選択しながら障害物を回避することの有用性について述べた．本実験環境において，強化学習を用いた障害物回避の有用性，SAP-netを用いて複数の障害物の環境下を含めた障害物回避の有用性，実機実装を用いての障害物回避が可能であることを示した．
\clearpage
\newpage
