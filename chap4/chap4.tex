\chapter{実験}
\thispagestyle{empty}
\label{chap4}
\minitoc

\newpage
\section{はじめに}
本章では，提案したアプローチの有用性を示すために行った実験について述べる．\ref{4 RLsim}節ではWebotsで障害物回避の強化学習シミュレーションによる実験の目的，実験条件，および実験結果について述べる．\ref{4 SAP-net}節では，SAP-netを実装したシミュレーション実験の目的，実験条件，および実験結果について述べる．\ref{4 zikki}節では学習結果を転移させた実機ロボットによる障害物回避の実験の目的，条件，および結果について述べる．

\clearpage

\section{障害物回避の強化学習実験}\label{4 RLsim}
\subsection{目的と実験条件}
事前実験として強化学習を用いて障害物を回避する実験を行った．事前実験の目的は障害物回避の知識を実機に転移させるためである．本実験の条件は以下の通りである．実験環境をFig．\ref{zikkennkannkyou1}に示す．
\begin{itemize}
\item 障害物がないパターン（1番）と前方に9パターン（2番〜10番）の合計10パターンを強化学習
\item 障害物の配置パターン1つにつき7000エピソードで学習
\item 強化学習はQ学習を使用
\item 行動価値はx座標，y座標，ロボットの向き，障害物までの角度，障害物までの最小距離を使用
\item ゴールにたどり着いたら正の報酬，障害物に衝突またはゴールから遠ざかる行動をしたら負の報酬を付与
\item 学習率は0.1，割引率は0.9，ボルツマン選択の温度定数は0.5
\end{itemize}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/zikkennkannkyou1}
	\caption{強化学習シミュレーションの環境}
	\label{zikkennkannkyou1}
\end{figure}
上記の条件・環境で強化学習を進め，エピソードごとのステップ数を分析して，障害物の回避策を効率的に実施するための知識を構築する．
\clearpage
\subsection{実験結果}
以下にFig.\ref{zikkennkannkyou1}の配置パターン(1番から10番)の強化学習の結果を示す．
\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/gakusyuu1}
	\caption{行動回数の推移を表した学習曲線（1番）}
	\label{gakusyuu1}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/gakusyuu2}
	\caption{行動回数の推移を表した学習曲線（2番）}
	\label{gakusyuu2}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/gakusyuu3}
	\caption{行動回数の推移を表した学習曲線（3番）}
	\label{gakusyuu3}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/gakusyuu4}
	\caption{行動回数の推移を表した学習曲線（4番）}
	\label{gakusyuu4}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/gakusyuu5}
	\caption{行動回数の推移を表した学習曲線（5番）}
	\label{gakusyuu5}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/gakusyuu6}
	\caption{行動回数の推移を表した学習曲線（6番）}
	\label{gakusyuu6}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/gakusyuu7}
	\caption{行動回数の推移を表した学習曲線（7番）}
	\label{gakusyuu7}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/gakusyuu8}
	\caption{行動回数の推移を表した学習曲線（8番）}
	\label{gakusyuu8}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/gakusyuu9}
	\caption{行動回数の推移を表した学習曲線（9番）}
	\label{gakusyuu9}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/gakusyuu10}
	\caption{行動回数の推移を表した学習曲線（10番）}
	\label{gakusyuu10}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/housyuu1}
	\caption{獲得報酬の推移を表した学習曲線（1番）}
	\label{housyuu1}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=140mm]{./chap4/chap4_fig/TLreward}
	\caption{獲得報酬の推移を表した学習曲線（2番）}
	\label{TLreward}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/housyuu3}
	\caption{行動回数の推移を表した学習曲線（3番）}
	\label{housyuu3}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/housyuu4}
	\caption{行動回数の推移を表した学習曲線（4番）}
	\label{housyuu4}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/housyuu5}
	\caption{行動回数の推移を表した学習曲線（5番）}
	\label{housyuu5}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/housyuu6}
	\caption{獲得報酬の推移を表した学習曲線（6番）}
	\label{housyuu6}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/housyuu7}
	\caption{獲得報酬の推移を表した学習曲線（7番）}
	\label{housyuu7}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/housyuu8}
	\caption{行動回数の推移を表した学習曲線（8番）}
	\label{housyuu8}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/housyuu9}
	\caption{行動回数の推移を表した学習曲線（9番）}
	\label{housyuu9}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/housyuu10}
	\caption{行動回数の推移を表した学習曲線（10番）}
	\label{housyuu10}
\end{figure} 
\clearpage
Fig.\ref{gakusyuu1}〜Fig.\ref{gakusyuu10}より，エピソード数（試行回数）が増えるにつれてステップ数（行動回数）が減少していることがわかる．このことから効率よく学習しながら，少ないステップ数で障害物を回避してゴールしていることを示している．Fig.\ref{housyuu1}〜Fig.\ref{housyuu10}より，エピソード数が増えるにつれて獲得報酬は正の値に収束傾向であることがわかる．このことから最適な行動戦略やポリシーを獲得し，報酬を最大化していることがわかる．以下に強化学習後のシミュレーションロボットを1エピソードだけ実行したので，移動軌跡をFig.\ref{kiseki1}〜Fig.\ref{kiseki10}に示す．\\
\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/kiseki1}
	\caption{強化学習後の移動軌跡（1番）}
	\label{kiseki1}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/TLkiseki}
	\caption{強化学習後の移動軌跡（2番）}
	\label{TLkiseki}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/kiseki3}
	\caption{強化学習後の移動軌跡（3番）}
	\label{kiseki3}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/kiseki4}
	\caption{強化学習後の移動軌跡（4番）}
	\label{kiseki4}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/kiseki5}
	\caption{強化学習後の移動軌跡（5番）}
	\label{kiseki5}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/kiseki6}
	\caption{強化学習後の移動軌跡（6番）}
	\label{kiseki6}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/kiseki7}
	\caption{強化学習後の移動軌跡（7番）}
	\label{kiseki7}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/kiseki8}
	\caption{強化学習後の移動軌跡（8番）}
	\label{kiseki8}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/kiseki9}
	\caption{強化学習後の移動軌跡（9番）}
	\label{kiseki9}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/kiseki10}
	\caption{強化学習後の移動軌跡（10番）}
	\label{kiseki10}
\end{figure}

\clearpage


Fig.\ref{kiseki1}〜Fig.\ref{kiseki10}より，それぞれの配置パターンで障害物を回避するような移動軌跡が示せた．よって障害物を回避する行動策を学習することができた．また10パターンの障害物回避の強化学習の結果から，Fig.\ref{zikkennkannkyou1}を参考に，Fig.\ref{POLICY}に示すような知識番号が書かれたSAP-netの知識ネットワークを作成した．例えば障害物が前方にない場合は，1番が選択されて直進をするような知識が選択される．障害物が5番の位置にある場合，知識番号5番が選択されて障害物を右に回避するような行動をとる．障害物が4番の位置にある場合，4番が選択されて左を回避するような行動をとる．知識番号間にある数値は知識間の重みである．重みの数値が小さいほど，次の行動をする時の知識選択の知識番号間にあるが選択されやすくなるようにした．例えば，現在3番が選択されている場合，2番か5番が選択されやすくなる．

\begin{figure}[h]
	\centering
	\includegraphics[width=100mm]{./chap4/chap4_fig/POLICY}
	\caption{SAP-netの知識ネットワークの概念図}
	\label{POLICY}
\end{figure}
\clearpage
\newpage
\section{SAP-netを実装したシミュレーション実験}\label{4 SAP-net}
\subsection{目的と実験条件}
前節にて述べた障害物回避の強化学習実験においては，Webotsを用いたシミュレーション環境でQ学習を基にした基本的な障害物回避の学習プロセスを実施し，ロボットが異なる障害物配置パターン下での効率的な回避行動を学習することができることを示した．これらの実験を通じて得られた知識は，ロボットが複雑な環境下での障害物回避能力を向上させるための基礎となる．本節では前節で得られた10個の知識（行動価値関数）をSAP-netの知識ネットワークの基盤として，さらに進んだ学習アルゴリズムの適用を行う．具体的には，SAP-netを通じて，ロボットが障害物までの角度と距離，ロボットの初期座標を考慮した上で，これまでに学習した知識（行動価値関数）と現在の環境との類似度を計算し，最適な行動選択を行うシミュレーションを実装する．シミュレーション環境は障害物を中心に配置する．
\subsection{実験結果}
Fig.\ref{SAP-net_sim}に行動回数の推移を表した学習曲線を，Fig.\ref{SAP-net_reward}に獲得報酬の推移を表した学習曲線を示す．
\newpage
\begin{figure}[h]
	\centering
	\includegraphics[width=110mm]{./chap4/chap4_fig/SAP-net_sim}
	\caption{行動回数の推移を表した学習曲線}
	\label{SAP-net_sim}
\end{figure}
\begin{figure}[h]
	\centering
	\includegraphics[width=110mm]{./chap4/chap4_fig/SAP-net_reward}
	\caption{獲得報酬の推移を表した学習曲線}
	\label{SAP-net_reward}
\end{figure}　
Fig.\ref{SAP-net_sim}より，知識（行動価値関数）を保持した状態なので，強化学習実験とは違ってより少ない行動でゴールしていることがわかる．Fig.\ref{SAP-net_reward}より，Webotsでのバグが生じたものの，ほぼ正の値に収束していることがわかる．よって知識選択型転移強化学習を用いることで障害物回避の有用性を示した．またFig.\ref{SAP-net_sim2}のような，障害物を2つ配置した場合の行動も検証したので，1エピソードだけ実行して，その時の移動軌跡をFig.\ref{SAP-net_kiseki}に示す．\\
\begin{figure}[h]
	\centering
	\includegraphics[width=110mm]{./chap4/chap4_fig/SAP-net_sim2}
	\caption{障害物を2つ配置した場合のシミュレーション環境}
	\label{SAP-net_simi2}
\end{figure}
\begin{figure}[h]
	\centering
	\includegraphics[width=110mm]{./chap4/chap4_fig/SAP-net_kiseki}
	\caption{障害物を2つ配置した場合の移動軌跡}
	\label{SAP-net_kiseki}
\end{figure}
2つの障害物をLIDARが認識しながら障害物を回避していることが示せた．よって知識選択型転移強化学習を用いることで複数の障害物も回避することの有用性を示した．
\clearpage
\section{実機実装と障害物回避性能の検証}\label{4 zikki}
\subsection{目的と実験条件}
本節では前節で得られた10個の知識（行動価値関数）とSAP-netを実機であるラズベリーパイマウスに転移させて，前節と同じような環境で障害物回避の実現を目的とする．具体的にはWebotsシミュレーションで得られた10個の知識（行動価値関数）とSAP-netを，ラズベリーパイマウスという実機に転移させることで，実際の物理環境で障害物回避戦略の有効性を検証する．この実機実装の過程には，シミュレーションから実機へのパラメータ調整を含む，ラズベリーパイマウスにSAP-netアルゴリズムと行動価値関数を組み込み，実世界での障害物回避の実現を目指す．実機実験では，シミュレーション環境と同等の条件下で物理的に障害物の配置を再現し，ラズベリーパイマウスが効率的に障害物を回避しつつ目標地点に到達できるかを評価する．使用する実機はFig.\ref{raspimouse}に示すように，Slamtec製RPLIDARを接続したRT製ラズベリーパイマウスを使用する．また，実験エリアをFig.\ref{zikeneria}示す．
\begin{figure}[h]
	\centering
	\includegraphics[width=60mm]{./chap4/chap4_fig/raspimouse}
	\caption{ラズベリーパイマウス}
	\label{raspimouse}
\end{figure}
\begin{figure}[h]
	\centering
	\includegraphics[width=60mm]{./chap4/chap4_fig/zikeneria}
	\caption{実験環境}
	\label{zikeneria}
\end{figure}
\subsection{実験結果}
Fig.\ref{syoumigikaihi}に障害物を右に回避した時の比較動合成画像を，Fig.\ref{syouhidarikaihi}に障害物を左に回避した時の比較動合成画像を示す．
\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/syoumigikaihi}
	\caption{障害物を右に回避した時の比較動合成画像}
	\label{syoumigikaihi}
\end{figure}
\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/syouhidarikaihi}
	\caption{障害物を右に回避した時の比較動合成画像}
	\label{syouhidarikaihi}
\end{figure}

Fig.\ref{syoumigikaihi}およびFig.\ref{syouhidarikaihi}より知識を選択しながら障害物を回避することに成功した．また，Fig.\ref{POLICY}から，ステップ数においての知識番号選択のグラフをFig.\ref{migikaihi}とFig.\ref{hidarikaihi}に示す．

\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/migikaihi}
	\caption{障害物を右に回避した時の知識選択の推移}
	\label{migikaihi}
\end{figure}
\begin{figure}[h]
	\centering
	\includegraphics[width=120mm]{./chap4/chap4_fig/hidarikaihi}
	\caption{障害物を左に回避した時の知識選択の推移}
	\label{hidarikaihi}
\end{figure}
Fig.\ref{migikaihi}より障害物を右に回避する際，Fig.\ref{POLICY}の3番や9番が選択されていることがわかる．3番や9番は障害物を右に回避する際に選択されやすくなる番号である．またFig.\ref{hidarikaihi}より障害物を左に回避する際，6番や8番が選択されていることがわかる．6番や8番は障害を左に回避する際に選択されやすくなる番号である．このことから環境情報を入力として知識を選択しながら障害物を回避していることが示せた．また，初期位置でのセットアップで方向の誤差により，進行方向が左右に少しずれて，選択する知識が変更になり，同時に障害物を回避する方向が変わったことが示せた．

\newpage
\clearpage
\section{おわりに}\label{4 last}
本章では，提案手法に基づいた実験の詳細について述べた．\ref{4 RLsim}節では，強化学習実験の目的，条件，および結果について述べた．\ref{4 SAP-net}節では，SAP-netを実装したシミュレーション実験の目的，条件，および結果について述べた．\ref{4 zikki}節では，強化学習で獲得した知識を実機に転移させ，またSAP-netを実装させることで，実環境で知識を選択しながら障害物を回避することの有用性について述べた．本実験環境において，強化学習を用いた障害物回避の有用性，SAP-netを用いて複数の障害物の環境下を含めた障害物回避の有用性，実機実装を用いての障害物回避が可能であることを示した．
\clearpage
\newpage
